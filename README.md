# ğŸ¤– LLM Evaluation Dashboard

A modular web application to **evaluate and audit GenAI / LLM-based decision systems** for key compliance attributes, starting with **Bias & Fairness**.

Built for rapid testing, validation, and documentation of AI systems â€“ especially in regulated contexts like **credit scoring**, **HR**, or **insurance**.

---

## âœ¨ Features

* âœ… Web-based UI to enter API credentials and select evaluation modules
* ğŸ“¦ Modular backend supporting multiple analysis types
* ğŸ“Š **Bias & Fairness Analysis**: Demographic parity, disparate impact, counterfactual fairness
* ğŸ“ Auto-generated, human-readable HTML report
* ğŸ“‚ Clear folder structure for results, logs, and configs
* ğŸ”’ Supports **Basic Auth**-protected APIs

---

## ğŸ“ Project Structure

```
credit_scoring_validator/
â”œâ”€â”€ app.py                     # Flask web app
â”œâ”€â”€ config.py                  # Config paths for logs, reports, etc.
â”œâ”€â”€ data/                      # Input test dataset (e.g. testdata.csv)
â”œâ”€â”€ api/
â”‚   â””â”€â”€ client.py              # Sends API requests
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ bias_fairness.py       # Bias evaluation logic
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ report_builder.py      # Generates HTML reports
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ report_template.html  # Report HTML template (Jinja2)
â”œâ”€â”€ results/
â”‚   â””â”€â”€ bias_fairness.jsonl    # Collected API responses
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...                    # Pytest test cases
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Install Requirements

```bash
pip install -r requirements.txt
```

(Requirements include Flask, pandas, requests, Jinja2, etc.)

### 2. Prepare Data

Add your test dataset to:

```
/data/testdata.csv
```

Ensure the format matches the expected input schema for your scoring API.

### 3. Run the Web App

```bash
python app.py
```

Open your browser and go to:

```
http://localhost:5000
```

---

## ğŸ§ª Modules

### âœ… Bias & Fairness

* **Demographic Parity** â€“ How equally are outcomes distributed across groups?
* **Disparate Impact Ratio** â€“ Ratio between most and least favored groups
* **Counterfactual Fairness** â€“ Would the decision change if only a protected attribute were altered?

### Coming Soon

* Accuracy
* Robustness
* Transparency
* Stability & Drift

---

## ğŸ” API Authentication

We support **Basic Auth** for protected endpoints.

You can enter:

* âœ… API URL
* ğŸ‘¤ Username
* ğŸ”‘ Password

These are used to call your LLM scoring API in real time.

---

## ğŸ“Š Reports

After each run, the tool generates an HTML report in:

```
results/report_bias_fairness.html
```

The report includes:

* ğŸ“ˆ Decision rates per group
* âš–ï¸ Fairness metrics
* ğŸš¨ Counterfactual violations
* ğŸ§  (Planned) GPT-based summaries & checklist evaluations

---

## ğŸ§ª Testing

To run unit tests:

```bash
pytest
```

Make sure all modules and the API client work as expected before deployment.

---

## ğŸ§  Why This Tool?

This project was developed to explore **responsible GenAI validation** for use cases like:

* ğŸ“‰ Credit Scoring
* ğŸ¢‘ HR Screening
* ğŸ“‹ Insurance Risk Assessment
* âš–ï¸ AI compliance with EU AI Act / ISO 42001

---

## ğŸ“¬ Feedback & Contributions

Have ideas, issues, or want to collaborate? Feel free to open an issue or reach out.

---

Â© 2025 â€” GenAI Validator Project. Built with â¤ï¸ for responsible AI.
